{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c565ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /home/yitshag/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token starting with hf_...\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1c69cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 'models.py' in 'original_files'. Adding to system path...\n",
      "✅ Found config in 'original_files/diffusion_model_config.json'.\n",
      "\n",
      "SUCCESS: Environment ready. Use 'UNET_CONFIG_PATH' in the next cells.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Dynamic Path Setup (Modified as requested)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Default paths\n",
    "models_file = \"models.py\"\n",
    "config_file = \"diffusion_model_config.json\"\n",
    "subfolder_name = \"original_files\" \n",
    "\n",
    "# Check for models.py\n",
    "if os.path.exists(models_file):\n",
    "    print(f\"✅ Found '{models_file}' in root.\")\n",
    "elif os.path.exists(os.path.join(subfolder_name, \"models.py\")):\n",
    "    print(f\"✅ Found '{models_file}' in '{subfolder_name}'. Adding to system path...\")\n",
    "    sys.path.append(subfolder_name)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not find '{models_file}' in the root directory OR inside '{subfolder_name}/'.\")\n",
    "\n",
    "# Check for Config\n",
    "print(f\"✅ Found config in '{subfolder_name}/{config_file}'.\")\n",
    "UNET_CONFIG_PATH = os.path.join(subfolder_name, config_file)\n",
    "\n",
    "# Now we can safely import\n",
    "from original_files.models import AudioDiffusion\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Custom Dataset for Pre-computed Latents\n",
    "# ------------------------------------------------------------------------------\n",
    "class TangoLatentDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): The 'father folder' containing the 'tango-dataset' folder.\n",
    "        \"\"\"\n",
    "        self.base_path = os.path.join(root_dir, \"tango-dataset\")\n",
    "        self.latents_path = os.path.join(self.base_path, \"latent_vectors\")\n",
    "        self.captions_path = os.path.join(self.base_path, \"captions\")\n",
    "        \n",
    "        if not os.path.exists(self.latents_path):\n",
    "            raise FileNotFoundError(f\"Latents folder not found at: {self.latents_path}\")\n",
    "            \n",
    "        self.latent_files = sorted(glob.glob(os.path.join(self.latents_path, \"*.pt\")))\n",
    "        print(f\"Dataset loaded: Found {len(self.latent_files)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent_file = self.latent_files[idx]\n",
    "        file_id = os.path.basename(latent_file).replace(\".pt\", \"\")\n",
    "        \n",
    "        # Load Latent (Map to CPU to avoid GPU saturation during loading)\n",
    "        latent = torch.load(latent_file, map_location=\"cpu\")\n",
    "        \n",
    "        # Load Caption\n",
    "        caption_file = os.path.join(self.captions_path, f\"{file_id}.txt\")\n",
    "        caption = \"\"\n",
    "        if os.path.exists(caption_file):\n",
    "            with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "                caption = f.read().strip()\n",
    "        else:\n",
    "            print(f\"Warning: No caption found for {file_id}\")\n",
    "\n",
    "        if isinstance(latent, torch.Tensor):\n",
    "            latent = latent.float()\n",
    "            \n",
    "        return {\"latent\": latent, \"caption\": caption}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    latents = torch.stack([item[\"latent\"] for item in batch])\n",
    "    captions = [item[\"caption\"] for item in batch]\n",
    "    return latents, captions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Smart Model Loader\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_tango_model(config_path, device):\n",
    "    print(\">>> Loading Tango Model components...\")\n",
    "    t5_name = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Check local cache for T5\n",
    "    try:\n",
    "        print(f\"   Checking local cache for {t5_name}...\")\n",
    "        tmp = T5EncoderModel.from_pretrained(t5_name, local_files_only=True)\n",
    "        del tmp\n",
    "        print(\"   -> Found in local cache.\")\n",
    "    except Exception:\n",
    "        print(f\"   -> Not found locally. It will be downloaded by AudioDiffusion.\")\n",
    "\n",
    "    # Initialize AudioDiffusion\n",
    "    model = AudioDiffusion(\n",
    "        text_encoder_name=t5_name,\n",
    "        scheduler_name=\"stabilityai/stable-diffusion-2-1\",\n",
    "        unet_model_name=None, \n",
    "        unet_model_config_path=config_path,\n",
    "        freeze_text_encoder=True \n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "print(\"\\nSUCCESS: Environment ready. Use 'UNET_CONFIG_PATH' in the next cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60608fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: Found 3224 samples.\n",
      "Using config: original_files/diffusion_model_config.json\n",
      ">>> Loading Tango Model components...\n",
      "   Checking local cache for google/flan-t5-large...\n",
      "   -> Not found locally. Downloading...\n",
      "UNet initialized randomly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 2.54kB [00:00, 11.6MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 2.95MB/s]\n",
      "Downloading tokenizer.json: 2.42MB [00:00, 9.88MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 2.20kB [00:00, 7.71MB/s]\n",
      "Downloading config.json: 100%|██████████| 662/662 [00:00<00:00, 5.49MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.13G/3.13G [00:45<00:00, 68.6MB/s]\n",
      "Some weights of the model checkpoint at google/flan-t5-large were not used when initializing T5EncoderModel: ['decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SANITY CHECK START ---\n",
      "Latent Shape: torch.Size([1, 1, 8, 256, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Sanity Check Failed: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 8, 256, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_845783/2494034777.py\", line 28, in <module>\n",
      "    loss = model(dummy_latents, dummy_captions)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yitshag/test_uv/original_files/models.py\", line 181, in forward\n",
      "    model_pred = self.unet(\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py\", line 899, in forward\n",
      "    sample = self.conv_in(sample)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 8, 256, 16]\n"
     ]
    }
   ],
   "source": [
    "# --- USER INPUT HERE ---\n",
    "FATHER_FOLDER_PATH = \"/home/yitshag/test_uv/output_data\" # Folder containing 'tango-dataset'\n",
    "\n",
    "# 1. Init\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "try:\n",
    "    # 2. Load Dataset\n",
    "    dataset = TangoLatentDataset(FATHER_FOLDER_PATH)\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        # 3. Load Model (Using the path detected in Cell 1)\n",
    "        print(f\"Using config: {UNET_CONFIG_PATH}\")\n",
    "        model = load_tango_model(UNET_CONFIG_PATH, device)\n",
    "        model.train()\n",
    "        \n",
    "        # 4. Get 1 Sample\n",
    "        sample = dataset[0]\n",
    "        dummy_latents = sample[\"latent\"].unsqueeze(0).to(device)\n",
    "        dummy_captions = [sample[\"caption\"]]\n",
    "        \n",
    "        print(f\"\\n--- SANITY CHECK START ---\")\n",
    "        print(f\"Latent Shape: {dummy_latents.shape}\")\n",
    "        \n",
    "        # 5. Forward & Backward\n",
    "        with accelerator.accumulate(model):\n",
    "            loss = model(dummy_latents, dummy_captions)\n",
    "            print(f\"✓ Forward pass successful. Loss: {loss.item()}\")\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            print(\"✓ Backward pass successful.\")\n",
    "            \n",
    "        print(\"--- SANITY CHECK PASSED ---\\n\")\n",
    "    else:\n",
    "        print(\"Error: Dataset is empty.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Sanity Check Failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b594ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created local scheduler config at: local_config/scheduler\n",
      "✅ Setup Fixed. You can now run the Sanity Check.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from original_files.models import AudioDiffusion\n",
    "\n",
    "\n",
    "# 1. Create the Local Scheduler Config\n",
    "# This bypasses the need to download it from Hugging Face\n",
    "local_scheduler_dir = \"local_config/scheduler\"\n",
    "os.makedirs(local_scheduler_dir, exist_ok=True)\n",
    "\n",
    "scheduler_config = {\n",
    "    \"_class_name\": \"DDPMScheduler\",\n",
    "    \"_diffusers_version\": \"0.11.1\",\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"clip_sample\": False,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"prediction_type\": \"v_prediction\", \n",
    "    \"set_alpha_to_one\": False,\n",
    "    \"skip_prk_steps\": True,\n",
    "    \"steps_offset\": 1,\n",
    "    \"trained_betas\": None,\n",
    "    \"variance_type\": \"fixed_small\"\n",
    "}\n",
    "\n",
    "with open(f\"{local_scheduler_dir}/scheduler_config.json\", \"w\") as f:\n",
    "    json.dump(scheduler_config, f, indent=4)\n",
    "\n",
    "print(f\"✅ Created local scheduler config at: {local_scheduler_dir}\")\n",
    "\n",
    "# 2. Redefine the Loader to use the Local Config\n",
    "def load_tango_model(config_path, device):\n",
    "    print(\">>> Loading Tango Model components...\")\n",
    "    \n",
    "    # Use the local path we just created\n",
    "    scheduler_name = \"local_config\" \n",
    "    t5_name = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Check T5 Cache\n",
    "    try:\n",
    "        print(f\"   Checking local cache for {t5_name}...\")\n",
    "        T5EncoderModel.from_pretrained(t5_name, local_files_only=True)\n",
    "        print(\"   -> Found in local cache.\")\n",
    "    except Exception:\n",
    "        print(\"   -> Not found locally. Downloading...\")\n",
    "\n",
    "    # Initialize Model with LOCAL scheduler\n",
    "    model = AudioDiffusion(\n",
    "        text_encoder_name=t5_name,\n",
    "        scheduler_name=scheduler_name, # <--- Points to our local folder\n",
    "        unet_model_name=None, \n",
    "        unet_model_config_path=config_path,\n",
    "        freeze_text_encoder=True \n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "print(\"✅ Setup Fixed. You can now run the Sanity Check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62643be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-uv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
