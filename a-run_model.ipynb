{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c565ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token starting with hf_...\n",
    "login(token=\"f_tObFkOdreZjyLBjXMrpbGShtBCmGKyYjdy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1c69cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Applied huggingface_hub compatibility patch (use_auth_token -> token)\n",
      "✅ Found 'models.py' in 'original_files'. Adding to system path...\n",
      "✅ Found config in 'original_files/diffusion_model_config.json'.\n",
      "✅ Created local scheduler config at: local_config/scheduler\n",
      "\n",
      "SUCCESS: Environment ready. Use 'UNET_CONFIG_PATH' in the next cells.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "# from diffusers import UNet2DConditionModel\n",
    "\n",
    "# Fix for newer diffusers versions where randn_tensor moved to diffusers.utils.torch_utils\n",
    "import diffusers.utils\n",
    "if not hasattr(diffusers.utils, \"randn_tensor\"):\n",
    "    from diffusers.utils.torch_utils import randn_tensor\n",
    "    diffusers.utils.randn_tensor = randn_tensor\n",
    "\n",
    "# Fix for transformers passing deprecated 'use_auth_token' to newer huggingface_hub\n",
    "import huggingface_hub\n",
    "_original_hf_hub_download = huggingface_hub.file_download.hf_hub_download\n",
    "\n",
    "@functools.wraps(_original_hf_hub_download)\n",
    "def _patched_hf_hub_download(*args, **kwargs):\n",
    "    if \"use_auth_token\" in kwargs:\n",
    "        kwargs[\"token\"] = kwargs.pop(\"use_auth_token\")\n",
    "    return _original_hf_hub_download(*args, **kwargs)\n",
    "\n",
    "huggingface_hub.file_download.hf_hub_download = _patched_hf_hub_download\n",
    "huggingface_hub.hf_hub_download = _patched_hf_hub_download\n",
    "\n",
    "print(\"✅ Applied huggingface_hub compatibility patch (use_auth_token -> token)\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Dynamic Path Setup (Modified as requested)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Default paths\n",
    "models_file = \"models.py\"\n",
    "config_file = \"diffusion_model_config.json\"\n",
    "subfolder_name = \"original_files\" \n",
    "\n",
    "# Check for models.py\n",
    "if os.path.exists(models_file):\n",
    "    print(f\"✅ Found '{models_file}' in root.\")\n",
    "elif os.path.exists(os.path.join(subfolder_name, \"models.py\")):\n",
    "    print(f\"✅ Found '{models_file}' in '{subfolder_name}'. Adding to system path...\")\n",
    "    sys.path.append(subfolder_name)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not find '{models_file}' in the root directory OR inside '{subfolder_name}/'.\")\n",
    "\n",
    "# Check for Config\n",
    "print(f\"✅ Found config in '{subfolder_name}/{config_file}'.\")\n",
    "UNET_CONFIG_PATH = os.path.join(subfolder_name, config_file)\n",
    "\n",
    "# Now we can safely import\n",
    "from original_files.models import AudioDiffusion\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1b. Create Local Scheduler Config (avoids downloading from gated HF repo)\n",
    "# ------------------------------------------------------------------------------\n",
    "LOCAL_SCHEDULER_DIR = \"local_config/scheduler\"\n",
    "os.makedirs(LOCAL_SCHEDULER_DIR, exist_ok=True)\n",
    "\n",
    "scheduler_config = {\n",
    "    \"_class_name\": \"DDPMScheduler\",\n",
    "    \"_diffusers_version\": \"0.11.1\",\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"clip_sample\": False,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"prediction_type\": \"v_prediction\",\n",
    "    \"set_alpha_to_one\": False,\n",
    "    \"skip_prk_steps\": True,\n",
    "    \"steps_offset\": 1,\n",
    "    \"trained_betas\": None,\n",
    "    \"variance_type\": \"fixed_small\"\n",
    "}\n",
    "\n",
    "with open(f\"{LOCAL_SCHEDULER_DIR}/scheduler_config.json\", \"w\") as f:\n",
    "    json.dump(scheduler_config, f, indent=4)\n",
    "\n",
    "print(f\"✅ Created local scheduler config at: {LOCAL_SCHEDULER_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Custom Dataset for Pre-computed Latents\n",
    "# ------------------------------------------------------------------------------\n",
    "class TangoLatentDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): The 'father folder' containing the 'tango-dataset' folder.\n",
    "        \"\"\"\n",
    "        self.base_path = os.path.join(root_dir, \"tango-dataset\")\n",
    "        self.latents_path = os.path.join(self.base_path, \"latent_vectors\")\n",
    "        self.captions_path = os.path.join(self.base_path, \"captions\")\n",
    "        \n",
    "        if not os.path.exists(self.latents_path):\n",
    "            raise FileNotFoundError(f\"Latents folder not found at: {self.latents_path}\")\n",
    "            \n",
    "        self.latent_files = sorted(glob.glob(os.path.join(self.latents_path, \"*.pt\")))\n",
    "        print(f\"Dataset loaded: Found {len(self.latent_files)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent_file = self.latent_files[idx]\n",
    "        file_id = os.path.basename(latent_file).replace(\".pt\", \"\")\n",
    "        \n",
    "        # Load Latent (Map to CPU to avoid GPU saturation during loading)\n",
    "        latent = torch.load(latent_file, map_location=\"cpu\")\n",
    "        \n",
    "        # Load Caption\n",
    "        caption_file = os.path.join(self.captions_path, f\"{file_id}.txt\")\n",
    "        caption = \"\"\n",
    "        if os.path.exists(caption_file):\n",
    "            with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "                caption = f.read().strip()\n",
    "        else:\n",
    "            print(f\"Warning: No caption found for {file_id}\")\n",
    "\n",
    "        if isinstance(latent, torch.Tensor):\n",
    "            latent = latent.float()\n",
    "            # Squeeze out the batch dimension saved by the VAE encoding step.\n",
    "            # Latents are saved as (1, 8, 256, 16) but should be (8, 256, 16)\n",
    "            # so that unsqueeze(0) / torch.stack in collate_fn produces correct 4D input.\n",
    "            while latent.dim() > 3:\n",
    "                latent = latent.squeeze(0)\n",
    "            \n",
    "        return {\"latent\": latent, \"caption\": caption}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    latents = torch.stack([item[\"latent\"] for item in batch])\n",
    "    captions = [item[\"caption\"] for item in batch]\n",
    "    return latents, captions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Smart Model Loader\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_tango_model(config_path, device):\n",
    "    print(\">>> Loading Tango Model components...\")\n",
    "    t5_name = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Use the local scheduler config to avoid 401 from gated HF repos\n",
    "    scheduler_name = \"local_config\"\n",
    "    \n",
    "    # Check local cache for T5\n",
    "    try:\n",
    "        print(f\"   Checking local cache for {t5_name}...\")\n",
    "        tmp = T5EncoderModel.from_pretrained(t5_name, local_files_only=True)\n",
    "        del tmp\n",
    "        print(\"   -> Found in local cache.\")\n",
    "    except Exception:\n",
    "        print(f\"   -> Not found locally. It will be downloaded by AudioDiffusion.\")\n",
    "\n",
    "    # Initialize AudioDiffusion\n",
    "    model = AudioDiffusion(\n",
    "        text_encoder_name=t5_name,\n",
    "        scheduler_name=scheduler_name,\n",
    "        unet_model_name=None, \n",
    "        unet_model_config_path=config_path,\n",
    "        freeze_text_encoder=True \n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "print(\"\\nSUCCESS: Environment ready. Use 'UNET_CONFIG_PATH' in the next cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60608fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: Found 3224 samples.\n",
      "Using config: original_files/diffusion_model_config.json\n",
      ">>> Loading Tango Model components...\n",
      "   Checking local cache for google/flan-t5-large...\n",
      "   -> Found in local cache.\n",
      "UNet initialized randomly.\n",
      "\n",
      "--- SANITY CHECK START ---\n",
      "Latent Shape: torch.Size([1, 8, 256, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forward pass successful. Loss: 1.102403163909912\n",
      "✓ Backward pass successful.\n",
      "--- SANITY CHECK PASSED ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- USER INPUT HERE ---\n",
    "FATHER_FOLDER_PATH = \"/home/yitshag/test_uv/output_data\" # Folder containing 'tango-dataset'\n",
    "\n",
    "# 1. Init\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "try:\n",
    "    # 2. Load Dataset\n",
    "    dataset = TangoLatentDataset(FATHER_FOLDER_PATH)\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        # 3. Load Model (Using the path detected in Cell 1)\n",
    "        print(f\"Using config: {UNET_CONFIG_PATH}\")\n",
    "        model = load_tango_model(UNET_CONFIG_PATH, device)\n",
    "        model.train()\n",
    "        \n",
    "        # 4. Get 1 Sample\n",
    "        sample = dataset[0]\n",
    "        dummy_latents = sample[\"latent\"].unsqueeze(0).to(device)\n",
    "        dummy_captions = [sample[\"caption\"]]\n",
    "        \n",
    "        print(f\"\\n--- SANITY CHECK START ---\")\n",
    "        print(f\"Latent Shape: {dummy_latents.shape}\")\n",
    "        \n",
    "        # 5. Forward & Backward\n",
    "        with accelerator.accumulate(model):\n",
    "            loss = model(dummy_latents, dummy_captions)\n",
    "            print(f\"✓ Forward pass successful. Loss: {loss.item()}\")\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            print(\"✓ Backward pass successful.\")\n",
    "            \n",
    "        print(\"--- SANITY CHECK PASSED ---\\n\")\n",
    "    else:\n",
    "        print(\"Error: Dataset is empty.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Sanity Check Failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b594ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created local scheduler config at: local_config/scheduler\n",
      "✅ Setup Fixed. You can now run the Sanity Check.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from original_files.models import AudioDiffusion\n",
    "\n",
    "\n",
    "# 1. Create the Local Scheduler Config\n",
    "# This bypasses the need to download it from Hugging Face\n",
    "local_scheduler_dir = \"local_config/scheduler\"\n",
    "os.makedirs(local_scheduler_dir, exist_ok=True)\n",
    "\n",
    "scheduler_config = {\n",
    "    \"_class_name\": \"DDPMScheduler\",\n",
    "    \"_diffusers_version\": \"0.11.1\",\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"clip_sample\": False,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"prediction_type\": \"v_prediction\", \n",
    "    \"set_alpha_to_one\": False,\n",
    "    \"skip_prk_steps\": True,\n",
    "    \"steps_offset\": 1,\n",
    "    \"trained_betas\": None,\n",
    "    \"variance_type\": \"fixed_small\"\n",
    "}\n",
    "\n",
    "with open(f\"{local_scheduler_dir}/scheduler_config.json\", \"w\") as f:\n",
    "    json.dump(scheduler_config, f, indent=4)\n",
    "\n",
    "print(f\"✅ Created local scheduler config at: {local_scheduler_dir}\")\n",
    "\n",
    "# 2. Redefine the Loader to use the Local Config\n",
    "def load_tango_model(config_path, device):\n",
    "    print(\">>> Loading Tango Model components...\")\n",
    "    \n",
    "    # Use the local path we just created\n",
    "    scheduler_name = \"local_config\" \n",
    "    t5_name = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Check T5 Cache\n",
    "    try:\n",
    "        print(f\"   Checking local cache for {t5_name}...\")\n",
    "        T5EncoderModel.from_pretrained(t5_name, local_files_only=True)\n",
    "        print(\"   -> Found in local cache.\")\n",
    "    except Exception:\n",
    "        print(\"   -> Not found locally. Downloading...\")\n",
    "\n",
    "    # Initialize Model with LOCAL scheduler\n",
    "    model = AudioDiffusion(\n",
    "        text_encoder_name=t5_name,\n",
    "        scheduler_name=scheduler_name, # <--- Points to our local folder\n",
    "        unet_model_name=None, \n",
    "        unet_model_config_path=config_path,\n",
    "        freeze_text_encoder=True \n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "print(\"✅ Setup Fixed. You can now run the Sanity Check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62643be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-uv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
