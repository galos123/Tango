{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c565ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token starting with hf_...\n",
    "login(token=\"f_tObFkOdreZjyLBjXMrpbGShtBCmGKyYjdy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1c69cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'xpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m T5EncoderModel, AutoTokenizer\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UNet2DConditionModel\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Fix for newer diffusers versions where randn_tensor moved to diffusers.utils.torch_utils\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n",
      "File \u001b[0;32m~/test_uv/.venv/lib/python3.10/site-packages/diffusers/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.36.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     DIFFUSERS_SLOW_IMPORT,\n\u001b[1;32m      7\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m      8\u001b[0m     _LazyModule,\n\u001b[1;32m      9\u001b[0m     is_accelerate_available,\n\u001b[1;32m     10\u001b[0m     is_bitsandbytes_available,\n\u001b[1;32m     11\u001b[0m     is_flax_available,\n\u001b[1;32m     12\u001b[0m     is_gguf_available,\n\u001b[1;32m     13\u001b[0m     is_k_diffusion_available,\n\u001b[1;32m     14\u001b[0m     is_librosa_available,\n\u001b[1;32m     15\u001b[0m     is_note_seq_available,\n\u001b[1;32m     16\u001b[0m     is_nvidia_modelopt_available,\n\u001b[1;32m     17\u001b[0m     is_onnx_available,\n\u001b[1;32m     18\u001b[0m     is_opencv_available,\n\u001b[1;32m     19\u001b[0m     is_optimum_quanto_available,\n\u001b[1;32m     20\u001b[0m     is_scipy_available,\n\u001b[1;32m     21\u001b[0m     is_sentencepiece_available,\n\u001b[1;32m     22\u001b[0m     is_torch_available,\n\u001b[1;32m     23\u001b[0m     is_torchao_available,\n\u001b[1;32m     24\u001b[0m     is_torchsde_available,\n\u001b[1;32m     25\u001b[0m     is_transformers_available,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n\u001b[1;32m     36\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguiders\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     ],\n\u001b[1;32m     65\u001b[0m }\n",
      "File \u001b[0;32m~/test_uv/.venv/lib/python3.10/site-packages/diffusers/utils/__init__.py:131\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOutput\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     check_peft_version,\n\u001b[1;32m    133\u001b[0m     delete_adapter_layers,\n\u001b[1;32m    134\u001b[0m     get_adapter_name,\n\u001b[1;32m    135\u001b[0m     get_peft_kwargs,\n\u001b[1;32m    136\u001b[0m     recurse_remove_peft_layers,\n\u001b[1;32m    137\u001b[0m     scale_lora_layers,\n\u001b[1;32m    138\u001b[0m     set_adapter_layers,\n\u001b[1;32m    139\u001b[0m     set_weights_and_activate_adapters,\n\u001b[1;32m    140\u001b[0m     unscale_lora_layers,\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpil_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PIL_INTERPOLATION, make_image_grid, numpy_to_pil, pt_to_pil\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m remote_decode\n",
      "File \u001b[0;32m~/test_uv/.venv/lib/python3.10/site-packages/diffusers/utils/peft_utils.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_peft_available, is_peft_version, is_torch_available\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m empty_device_cache\n\u001b[1;32m     29\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "File \u001b[0;32m~/test_uv/.venv/lib/python3.10/site-packages/diffusers/utils/torch_utils.py:33\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fftn, fftshift, ifftn, ifftshift\n\u001b[1;32m     30\u001b[0m BACKEND_SUPPORTS_TRAINING \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m     31\u001b[0m BACKEND_EMPTY_CACHE \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache,\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpu\u001b[49m\u001b[38;5;241m.\u001b[39mempty_cache,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m }\n\u001b[1;32m     38\u001b[0m BACKEND_DEVICE_COUNT \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mdevice_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     45\u001b[0m BACKEND_MANUAL_SEED \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed,\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mmanual_seed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmanual_seed,\n\u001b[1;32m     51\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'xpu'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# Fix for newer diffusers versions where randn_tensor moved to diffusers.utils.torch_utils\n",
    "import diffusers.utils\n",
    "if not hasattr(diffusers.utils, \"randn_tensor\"):\n",
    "    from diffusers.utils.torch_utils import randn_tensor\n",
    "    diffusers.utils.randn_tensor = randn_tensor\n",
    "\n",
    "# Fix for transformers passing deprecated 'use_auth_token' to newer huggingface_hub\n",
    "import huggingface_hub\n",
    "_original_hf_hub_download = huggingface_hub.file_download.hf_hub_download\n",
    "\n",
    "@functools.wraps(_original_hf_hub_download)\n",
    "def _patched_hf_hub_download(*args, **kwargs):\n",
    "    if \"use_auth_token\" in kwargs:\n",
    "        kwargs[\"token\"] = kwargs.pop(\"use_auth_token\")\n",
    "    return _original_hf_hub_download(*args, **kwargs)\n",
    "\n",
    "huggingface_hub.file_download.hf_hub_download = _patched_hf_hub_download\n",
    "huggingface_hub.hf_hub_download = _patched_hf_hub_download\n",
    "\n",
    "print(\"✅ Applied huggingface_hub compatibility patch (use_auth_token -> token)\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Dynamic Path Setup (Modified as requested)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Default paths\n",
    "models_file = \"models.py\"\n",
    "config_file = \"diffusion_model_config.json\"\n",
    "subfolder_name = \"original_files\" \n",
    "\n",
    "# Check for models.py\n",
    "if os.path.exists(models_file):\n",
    "    print(f\"✅ Found '{models_file}' in root.\")\n",
    "elif os.path.exists(os.path.join(subfolder_name, \"models.py\")):\n",
    "    print(f\"✅ Found '{models_file}' in '{subfolder_name}'. Adding to system path...\")\n",
    "    sys.path.append(subfolder_name)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not find '{models_file}' in the root directory OR inside '{subfolder_name}/'.\")\n",
    "\n",
    "# Check for Config\n",
    "print(f\"✅ Found config in '{subfolder_name}/{config_file}'.\")\n",
    "UNET_CONFIG_PATH = os.path.join(subfolder_name, config_file)\n",
    "\n",
    "# Now we can safely import\n",
    "from original_files.models import AudioDiffusion\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1b. Create Local Scheduler Config (avoids downloading from gated HF repo)\n",
    "# ------------------------------------------------------------------------------\n",
    "LOCAL_SCHEDULER_DIR = \"local_config/scheduler\"\n",
    "os.makedirs(LOCAL_SCHEDULER_DIR, exist_ok=True)\n",
    "\n",
    "scheduler_config = {\n",
    "    \"_class_name\": \"DDPMScheduler\",\n",
    "    \"_diffusers_version\": \"0.11.1\",\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"clip_sample\": False,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"prediction_type\": \"v_prediction\",\n",
    "    \"set_alpha_to_one\": False,\n",
    "    \"skip_prk_steps\": True,\n",
    "    \"steps_offset\": 1,\n",
    "    \"trained_betas\": None,\n",
    "    \"variance_type\": \"fixed_small\"\n",
    "}\n",
    "\n",
    "with open(f\"{LOCAL_SCHEDULER_DIR}/scheduler_config.json\", \"w\") as f:\n",
    "    json.dump(scheduler_config, f, indent=4)\n",
    "\n",
    "print(f\"✅ Created local scheduler config at: {LOCAL_SCHEDULER_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Custom Dataset for Pre-computed Latents\n",
    "# ------------------------------------------------------------------------------\n",
    "class TangoLatentDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): The 'father folder' containing the 'tango-dataset' folder.\n",
    "        \"\"\"\n",
    "        self.base_path = os.path.join(root_dir, \"tango-dataset\")\n",
    "        self.latents_path = os.path.join(self.base_path, \"latent_vectors\")\n",
    "        self.captions_path = os.path.join(self.base_path, \"captions\")\n",
    "        \n",
    "        if not os.path.exists(self.latents_path):\n",
    "            raise FileNotFoundError(f\"Latents folder not found at: {self.latents_path}\")\n",
    "            \n",
    "        self.latent_files = sorted(glob.glob(os.path.join(self.latents_path, \"*.pt\")))\n",
    "        print(f\"Dataset loaded: Found {len(self.latent_files)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent_file = self.latent_files[idx]\n",
    "        file_id = os.path.basename(latent_file).replace(\".pt\", \"\")\n",
    "        \n",
    "        # Load Latent (Map to CPU to avoid GPU saturation during loading)\n",
    "        latent = torch.load(latent_file, map_location=\"cpu\")\n",
    "        \n",
    "        # Load Caption\n",
    "        caption_file = os.path.join(self.captions_path, f\"{file_id}.txt\")\n",
    "        caption = \"\"\n",
    "        if os.path.exists(caption_file):\n",
    "            with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "                caption = f.read().strip()\n",
    "        else:\n",
    "            print(f\"Warning: No caption found for {file_id}\")\n",
    "\n",
    "        if isinstance(latent, torch.Tensor):\n",
    "            latent = latent.float()\n",
    "            # Squeeze out the batch dimension saved by the VAE encoding step.\n",
    "            # Latents are saved as (1, 8, 256, 16) but should be (8, 256, 16)\n",
    "            # so that unsqueeze(0) / torch.stack in collate_fn produces correct 4D input.\n",
    "            while latent.dim() > 3:\n",
    "                latent = latent.squeeze(0)\n",
    "            \n",
    "        return {\"latent\": latent, \"caption\": caption}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    latents = torch.stack([item[\"latent\"] for item in batch])\n",
    "    captions = [item[\"caption\"] for item in batch]\n",
    "    return latents, captions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Smart Model Loader\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_tango_model(config_path, device):\n",
    "    print(\">>> Loading Tango Model components...\")\n",
    "    t5_name = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Use the local scheduler config to avoid 401 from gated HF repos\n",
    "    scheduler_name = \"local_config\"\n",
    "    \n",
    "    # Check local cache for T5\n",
    "    try:\n",
    "        print(f\"   Checking local cache for {t5_name}...\")\n",
    "        tmp = T5EncoderModel.from_pretrained(t5_name, local_files_only=True)\n",
    "        del tmp\n",
    "        print(\"   -> Found in local cache.\")\n",
    "    except Exception:\n",
    "        print(f\"   -> Not found locally. It will be downloaded by AudioDiffusion.\")\n",
    "\n",
    "    # Initialize AudioDiffusion\n",
    "    model = AudioDiffusion(\n",
    "        text_encoder_name=t5_name,\n",
    "        scheduler_name=scheduler_name,\n",
    "        unet_model_name=None, \n",
    "        unet_model_config_path=config_path,\n",
    "        freeze_text_encoder=True \n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "print(\"\\nSUCCESS: Environment ready. Use 'UNET_CONFIG_PATH' in the next cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60608fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: Found 3224 samples.\n",
      "Using config: original_files/diffusion_model_config.json\n",
      ">>> Loading Tango Model components...\n",
      "   Checking local cache for google/flan-t5-large...\n",
      "   -> Not found locally. Downloading...\n",
      "UNet initialized randomly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 2.54kB [00:00, 11.6MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 2.95MB/s]\n",
      "Downloading tokenizer.json: 2.42MB [00:00, 9.88MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 2.20kB [00:00, 7.71MB/s]\n",
      "Downloading config.json: 100%|██████████| 662/662 [00:00<00:00, 5.49MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.13G/3.13G [00:45<00:00, 68.6MB/s]\n",
      "Some weights of the model checkpoint at google/flan-t5-large were not used when initializing T5EncoderModel: ['decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SANITY CHECK START ---\n",
      "Latent Shape: torch.Size([1, 1, 8, 256, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Sanity Check Failed: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 8, 256, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_845783/2494034777.py\", line 28, in <module>\n",
      "    loss = model(dummy_latents, dummy_captions)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yitshag/test_uv/original_files/models.py\", line 181, in forward\n",
      "    model_pred = self.unet(\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py\", line 899, in forward\n",
      "    sample = self.conv_in(sample)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/yitshag/test_uv/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 8, 256, 16]\n"
     ]
    }
   ],
   "source": [
    "# --- USER INPUT HERE ---\n",
    "FATHER_FOLDER_PATH = \"/home/yitshag/test_uv/output_data\" # Folder containing 'tango-dataset'\n",
    "\n",
    "# 1. Init\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "try:\n",
    "    # 2. Load Dataset\n",
    "    dataset = TangoLatentDataset(FATHER_FOLDER_PATH)\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        # 3. Load Model (Using the path detected in Cell 1)\n",
    "        print(f\"Using config: {UNET_CONFIG_PATH}\")\n",
    "        model = load_tango_model(UNET_CONFIG_PATH, device)\n",
    "        model.train()\n",
    "        \n",
    "        # 4. Get 1 Sample\n",
    "        sample = dataset[0]\n",
    "        dummy_latents = sample[\"latent\"].unsqueeze(0).to(device)\n",
    "        dummy_captions = [sample[\"caption\"]]\n",
    "        \n",
    "        print(f\"\\n--- SANITY CHECK START ---\")\n",
    "        print(f\"Latent Shape: {dummy_latents.shape}\")\n",
    "        \n",
    "        # 5. Forward & Backward\n",
    "        with accelerator.accumulate(model):\n",
    "            loss = model(dummy_latents, dummy_captions)\n",
    "            print(f\"✓ Forward pass successful. Loss: {loss.item()}\")\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            print(\"✓ Backward pass successful.\")\n",
    "            \n",
    "        print(\"--- SANITY CHECK PASSED ---\\n\")\n",
    "    else:\n",
    "        print(\"Error: Dataset is empty.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Sanity Check Failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b594ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created local scheduler config at: local_config/scheduler\n",
      "✅ Setup Fixed. You can now run the Sanity Check.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from original_files.models import AudioDiffusion\n",
    "\n",
    "\n",
    "# 1. Create the Local Scheduler Config\n",
    "# This bypasses the need to download it from Hugging Face\n",
    "local_scheduler_dir = \"local_config/scheduler\"\n",
    "os.makedirs(local_scheduler_dir, exist_ok=True)\n",
    "\n",
    "scheduler_config = {\n",
    "    \"_class_name\": \"DDPMScheduler\",\n",
    "    \"_diffusers_version\": \"0.11.1\",\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"clip_sample\": False,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"prediction_type\": \"v_prediction\", \n",
    "    \"set_alpha_to_one\": False,\n",
    "    \"skip_prk_steps\": True,\n",
    "    \"steps_offset\": 1,\n",
    "    \"trained_betas\": None,\n",
    "    \"variance_type\": \"fixed_small\"\n",
    "}\n",
    "\n",
    "with open(f\"{local_scheduler_dir}/scheduler_config.json\", \"w\") as f:\n",
    "    json.dump(scheduler_config, f, indent=4)\n",
    "\n",
    "print(f\"✅ Created local scheduler config at: {local_scheduler_dir}\")\n",
    "\n",
    "# 2. Redefine the Loader to use the Local Config\n",
    "def load_tango_model(config_path, device):\n",
    "    print(\">>> Loading Tango Model components...\")\n",
    "    \n",
    "    # Use the local path we just created\n",
    "    scheduler_name = \"local_config\" \n",
    "    t5_name = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Check T5 Cache\n",
    "    try:\n",
    "        print(f\"   Checking local cache for {t5_name}...\")\n",
    "        T5EncoderModel.from_pretrained(t5_name, local_files_only=True)\n",
    "        print(\"   -> Found in local cache.\")\n",
    "    except Exception:\n",
    "        print(\"   -> Not found locally. Downloading...\")\n",
    "\n",
    "    # Initialize Model with LOCAL scheduler\n",
    "    model = AudioDiffusion(\n",
    "        text_encoder_name=t5_name,\n",
    "        scheduler_name=scheduler_name, # <--- Points to our local folder\n",
    "        unet_model_name=None, \n",
    "        unet_model_config_path=config_path,\n",
    "        freeze_text_encoder=True \n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "print(\"✅ Setup Fixed. You can now run the Sanity Check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62643be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-uv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
